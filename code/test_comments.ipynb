{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c92-Km1bKKBW"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"fine-tuned-bert-imdb\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"fine-tuned-bert-imdb\")\n",
        "\n",
        "# Define a function to perform sentiment analysis\n",
        "def classify_comments(comments):\n",
        "    inputs = tokenizer(comments, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "    return predictions\n",
        "\n",
        "# Labels for classification\n",
        "labels = ['negative', 'positive']\n",
        "\n",
        "# Directory where the CSV files are stored\n",
        "directory = '/content'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Process each CSV file in the directory one at a time\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith('.csv'):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "        # Read the CSV file\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Clean column names\n",
        "        df.columns = df.columns.str.replace(r'^=\"|\"$', '', regex=True)\n",
        "        print(\"Cleaned Columns found:\", df.columns.tolist())\n",
        "\n",
        "        # Check if 'prediction' column exists\n",
        "        if 'prediction' not in df.columns:\n",
        "            # Check if 'Comment Text' column exists\n",
        "            if 'Comment Text' in df.columns:\n",
        "                # Extract comments\n",
        "                comments = df['Comment Text'].tolist()\n",
        "\n",
        "                # Classify comments in batches to manage memory usage\n",
        "                batch_size = 32\n",
        "                results = []\n",
        "                for i in range(0, len(comments), batch_size):\n",
        "                    batch_comments = comments[i:i + batch_size]\n",
        "                    predictions = classify_comments(batch_comments)\n",
        "                    results.extend([labels[prediction] for prediction in predictions])\n",
        "\n",
        "                # Add the results to a new column\n",
        "                df['prediction'] = results\n",
        "\n",
        "                # Save the updated DataFrame back to the CSV file\n",
        "                df.to_csv(file_path, index=False)\n",
        "\n",
        "                # Print comments and their sentiments\n",
        "                for comment, result in zip(comments, results):\n",
        "                    print(f\"Comment: {comment}\")\n",
        "                    print(f\"Sentiment: {result}\")\n",
        "            else:\n",
        "                print(f\"'Comment Text' column not found in {filename}\")\n",
        "        else:\n",
        "            print(f\"'prediction' column already exists in {filename}, skipping file.\")\n",
        "\n",
        "        # Clear variables to free up memory\n",
        "        del df\n",
        "        del comments\n",
        "        del results\n",
        "        torch.cuda.empty_cache()  # If using a GPU\n"
      ],
      "metadata": {
        "id": "TAK4KhWfKQbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tgcEmAJgLfj9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}